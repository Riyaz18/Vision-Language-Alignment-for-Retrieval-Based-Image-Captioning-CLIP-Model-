# Vision-Language-Alignment-for-Retrieval-Based-Image-Captioning-CLIP-Model-
This project leverages the powerful **CLIP (Contrastive Language-Image Pre-training)** model from OpenAI to calculate the semantic and visual coherence between an image and a list of candidate captions, thereby solving the image-to-text retrieval problem.
