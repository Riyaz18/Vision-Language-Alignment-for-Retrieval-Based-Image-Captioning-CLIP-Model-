{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16e6f36e-20a7-402c-89f2-a025076848bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bee3342a7564d068b0b81170e9f10ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Riyaz\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Riyaz\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec3dcfb87bb4342920646c758e54fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2169f43ac7c2466a990fb08e44136d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2e485a11354e58a0cd28f45d1d2440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7866107e92e848268c2632ff41b145ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f2af5321b74b25be0b4cf525b3d8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50de2cff06f94b46afa1b2c6d2a45d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593a049230f84384974831baa1269261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Best Captions:\n",
      "1. Your moment of peace, your daily indulgence. (Similarity: 0.2538)\n",
      "2. Embrace the moment, one cup at a time. (Similarity: 0.2515)\n",
      "3. Taste the world, one sip at a time. (Similarity: 0.2495)\n",
      "4. Unwind and enjoy. (Similarity: 0.2487)\n",
      "5. Savor the moment of peace. (Similarity: 0.2486)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359cb37c87db4ebd9f51ba8ea9daf605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Candidate captions ===\n",
    "candidate_captions = [\n",
    "    \"Trees, Travel and Tea!\",\n",
    "    \"A refreshing beverage.\",\n",
    "    \"A moment of indulgence.\",\n",
    "    \"The perfect thirst quencher.\",\n",
    "    \"Your daily dose of delight.\",\n",
    "    \"Taste the tradition.\",\n",
    "    \"Savor the flavor.\",\n",
    "    \"Refresh and rejuvenate.\",\n",
    "    \"Unwind and enjoy.\",\n",
    "    \"The taste of home.\",\n",
    "    \"A treat for your senses.\",\n",
    "    \"A taste of adventure.\",\n",
    "    \"A moment of bliss.\",\n",
    "    \"Your travel companion.\",\n",
    "    \"Fuel for your journey.\",\n",
    "    \"The essence of nature.\",\n",
    "    \"The warmth of comfort.\",\n",
    "    \"A sip of happiness.\",\n",
    "    \"Pure indulgence.\",\n",
    "    \"Quench your thirst, ignite your spirit.\",\n",
    "    \"Awaken your senses, embrace the moment.\",\n",
    "    \"The taste of faraway lands.\",\n",
    "    \"A taste of home, wherever you are.\",\n",
    "    \"Your daily dose of delight.\",\n",
    "    \"Your moment of serenity.\",\n",
    "    \"The perfect pick-me-up.\",\n",
    "    \"The perfect way to unwind.\",\n",
    "    \"Taste the difference.\",\n",
    "    \"Experience the difference.\",\n",
    "    \"A refreshing escape.\",\n",
    "    \"A delightful escape.\",\n",
    "    \"The taste of tradition, the spirit of adventure.\",\n",
    "    \"The warmth of home, the joy of discovery.\",\n",
    "    \"Your passport to flavor.\",\n",
    "    \"Your ticket to tranquility.\",\n",
    "    \"Sip, savor, and explore.\",\n",
    "    \"Indulge, relax, and rejuvenate.\",\n",
    "    \"The taste of wanderlust.\",\n",
    "    \"The comfort of home.\",\n",
    "    \"A journey for your taste buds.\",\n",
    "    \"A haven for your senses.\",\n",
    "    \"Your refreshing companion.\",\n",
    "    \"Your delightful escape.\",\n",
    "    \"Taste the world, one sip at a time.\",\n",
    "    \"Embrace the moment, one cup at a time.\",\n",
    "    \"The essence of exploration.\",\n",
    "    \"The comfort of connection.\",\n",
    "    \"Quench your thirst for adventure.\",\n",
    "    \"Savor the moment of peace.\",\n",
    "    \"The taste of discovery.\",\n",
    "    \"The warmth of belonging.\",\n",
    "    \"Your travel companion, your daily delight.\",\n",
    "    \"Your moment of peace, your daily indulgence.\",\n",
    "    \"The spirit of exploration, the comfort of home.\",\n",
    "    \"The joy of discovery, the warmth of connection.\",\n",
    "    \"Sip, savor, and set off on an adventure.\",\n",
    "    \"Indulge, relax, and find your peace.\",\n",
    "    \"A delightful beverage.\",\n",
    "    \"A moment of relaxation.\",\n",
    "    \"The perfect way to start your day.\",\n",
    "    \"The perfect way to end your day.\",\n",
    "    \"A treat for yourself.\",\n",
    "    \"Something to savor.\",\n",
    "    \"A moment of calm.\",\n",
    "    \"A taste of something special.\",\n",
    "    \"A refreshing pick-me-up.\",\n",
    "    \"A comforting drink.\",\n",
    "    \"A taste of adventure.\",\n",
    "    \"A moment of peace.\",\n",
    "    \"A small indulgence.\",\n",
    "    \"A daily ritual.\",\n",
    "    \"A way to connect with others.\",\n",
    "    \"A way to connect with yourself.\",\n",
    "    \"A taste of home.\",\n",
    "    \"A taste of something new.\",\n",
    "    \"A moment to enjoy.\",\n",
    "    \"A moment to remember.\"\n",
    "]\n",
    "\n",
    "# === Image preprocessing ===\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    return inputs, processor\n",
    "\n",
    "# === Generate image embeddings ===\n",
    "def generate_image_embeddings(inputs):\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    return image_features, model\n",
    "\n",
    "# === Match captions to image ===\n",
    "def match_captions(image_features, captions, clip_model, processor):\n",
    "    text_inputs = processor(text=captions, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = clip_model.get_text_features(**text_inputs)\n",
    "\n",
    "    image_features_np = image_features.detach().cpu().numpy()\n",
    "    text_features_np = text_features.detach().cpu().numpy()\n",
    "\n",
    "    similarities = cosine_similarity(image_features_np, text_features_np)\n",
    "    best_indices = similarities.argsort(axis=1)[0][::-1]\n",
    "    best_captions = [captions[i] for i in best_indices]\n",
    "\n",
    "    return best_captions, similarities[0][best_indices].tolist()\n",
    "\n",
    "# === Combine everything into one function ===\n",
    "def image_captioning(image_path, candidate_captions):\n",
    "    inputs, processor = load_and_preprocess_image(image_path)\n",
    "    image_features, clip_model = generate_image_embeddings(inputs)\n",
    "    best_captions, similarities = match_captions(image_features, candidate_captions, clip_model, processor)\n",
    "    return best_captions, similarities\n",
    "\n",
    "# === Run for your image ===\n",
    "best_captions, similarities = image_captioning(\"aman.png\", candidate_captions)\n",
    "\n",
    "# === Display top 5 captions ===\n",
    "top_n = min(5, len(best_captions))\n",
    "top_best_captions = best_captions[:top_n]\n",
    "top_similarities = similarities[:top_n]\n",
    "\n",
    "print(\"Top 5 Best Captions:\")\n",
    "for i, (caption, similarity) in enumerate(zip(top_best_captions, top_similarities)):\n",
    "    print(f\"{i+1}. {caption} (Similarity: {similarity:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07151df8-58a9-4838-a0da-f3580f7c8395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
